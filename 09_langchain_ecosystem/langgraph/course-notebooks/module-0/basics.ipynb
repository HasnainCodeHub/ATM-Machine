{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HasnainCodeHub/ATM-Machine/blob/main/09_langchain_ecosystem/langgraph/course-notebooks/module-0/basics.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ef597741-3211-4ecc-92f7-f58023ee237e",
      "metadata": {
        "id": "ef597741-3211-4ecc-92f7-f58023ee237e"
      },
      "source": [
        "# LangChain Academy\n",
        "\n",
        "Welcome to LangChain Academy!\n",
        "\n",
        "## Context\n",
        "\n",
        "At LangChain, we aim to make it easy to build LLM applications. One type of LLM application you can build is an agent. There’s a lot of excitement around building agents because they can automate a wide range of tasks that were previously impossible.\n",
        "\n",
        "In practice though, it is incredibly difficult to build systems that reliably execute on these tasks. As we’ve worked with our users to put agents into production, we’ve learned that more control is often necessary. You might need an agent to always call a specific tool first or use different prompts based on its state.\n",
        "\n",
        "To tackle this problem, we’ve built [LangGraph](https://langchain-ai.github.io/langgraph/) — a framework for building agent and multi-agent applications. Separate from the LangChain package, LangGraph’s core design philosophy is to help developers add better precision and control into agent workflows, suitable for the complexity of real-world systems.\n",
        "\n",
        "## Course Structure\n",
        "\n",
        "The course is structured as a set of modules, with each module focused on a particular theme related to LangGraph. You will see a folder for each module, which contains a series of notebooks. A video will accompany each notebook to help walk through the concepts, but the notebooks are also stand-alone, meaning that they contain explanations and can be viewed independent of the videos. Each module folder also contains a `studio` folder, which contains a set of graphs that can be loaded into [LangGraph Studio](https://github.com/langchain-ai/langgraph-studio), our IDE for building LangGraph applications.\n",
        "\n",
        "## Setup\n",
        "\n",
        "Before you begin, please follow the instructions in the `README` to create an environment and install dependencies.\n",
        "\n",
        "## Chat models\n",
        "\n",
        "In this course, we'll be using [Chat Models](https://python.langchain.com/v0.2/docs/concepts/#chat-models), which do a few things take a sequence of messages as inputs and return chat messages as outputs. LangChain does not host any Chat Models, rather we rely on third party integrations. [Here](https://python.langchain.com/v0.2/docs/integrations/chat/) is a list of 3rd party chat model integrations within LangChain! By default, the course with use [GEMINI_API_KEY](https://ai.google.dev/gemini-api/docs/api-key/) because it is both popular and performant. As noted, please ensure that you have an `GEMINI_API_KEY`.\n",
        "\n",
        "Let's check that your `GEMINI_API_KEY` is set and, if not, you will be asked to enter it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "0f9a52c8",
      "metadata": {
        "id": "0f9a52c8"
      },
      "outputs": [],
      "source": [
        "%%capture --no-stderr\n",
        "!pip install -q langchain_google_genai langchain_core langchain_community tavily-python"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "from google.colab import userdata\n",
        "GEMINI_API_KEY = userdata.get('GEMINI_API_KEY')\n",
        "llm = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-1.5-flash\",\n",
        "    api_key=GEMINI_API_KEY,\n",
        "    temperature=0\n",
        ")\n"
      ],
      "metadata": {
        "id": "Ch7HciDZXh3F"
      },
      "id": "Ch7HciDZXh3F",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result  = llm.invoke(\"What is the Capital Of Pakistan?\")\n",
        "result"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KKnOnfj3YZYT",
        "outputId": "4965d3fa-22e3-42fe-e199-0a8d9a5ffa84"
      },
      "id": "KKnOnfj3YZYT",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='Islamabad\\n', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []}, id='run-e6a02266-c39a-4377-be6e-ca6f88295b23-0', usage_metadata={'input_tokens': 8, 'output_tokens': 3, 'total_tokens': 11, 'input_token_details': {'cache_read': 0}})"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "28450d1b",
      "metadata": {
        "id": "28450d1b"
      },
      "source": [
        "Chat models in LangChain have a number of [default methods](https://python.langchain.com/v0.2/docs/concepts/#runnable-interface). For the most part, we'll be using:\n",
        "\n",
        "* `stream`: stream back chunks of the response\n",
        "* `invoke`: call the chain on an input\n",
        "\n",
        "And, as mentioned, chat models take [messages](https://python.langchain.com/v0.2/docs/concepts/#messages) as input. Messages have a role (that describes who is saying the message) and a content property. We'll be talking a lot more about this later, but here let's just show the basics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "b1280e1b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b1280e1b",
        "outputId": "f2cdd79a-587b-4804-f399-5e61bd9ed57a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='There are several ways to learn LangChain, depending on your learning style and prior experience:\\n\\n**1. Official Documentation:** The best place to start is the official LangChain documentation.  It\\'s well-structured and provides comprehensive tutorials and examples.  Look for their \"Getting Started\" guides and work your way through the examples.\\n\\n**2. Tutorials and Blog Posts:** Many tutorials and blog posts are available online that cover various aspects of LangChain. Search for \"LangChain tutorial\" or \"LangChain example\" on Google, YouTube, or Medium.  Look for tutorials that focus on specific use cases that interest you, such as building a chatbot or a question-answering system.\\n\\n**3. GitHub Repository:** Explore the LangChain GitHub repository.  You can find the source code, contribute to the project, and see how different components are implemented.  Reading the code can be a great way to deepen your understanding.\\n\\n**4. Example Projects:**  The LangChain documentation and GitHub repository contain many example projects.  Start by running and modifying these examples to understand how different parts of the framework work together.  Try to break them and see what happens – this is a great way to learn.\\n\\n**5. Community and Forums:** Engage with the LangChain community.  Ask questions on forums like Stack Overflow or join the LangChain Discord server (if one exists).  This is a great way to get help and learn from others\\' experiences.\\n\\n**6. Courses (Future Possibility):** While there aren\\'t many dedicated LangChain courses yet, as the framework is relatively new, you might find relevant courses on broader topics like large language models (LLMs) or building AI applications.  These courses might include sections on LangChain or provide a foundation that makes learning LangChain easier.\\n\\n\\n**To get started quickly, I recommend:**\\n\\n* **Familiarize yourself with the basic concepts:** Understand what LLMs are and how they work.  Having a basic understanding of Python programming is also essential.\\n* **Follow the official \"Getting Started\" guide:** This will walk you through setting up LangChain and running a simple example.\\n* **Choose a specific project:**  Instead of trying to learn everything at once, focus on a specific application you want to build (e.g., a chatbot, a summarizer, a question-answering system).  This will give you a clear goal and help you focus your learning.\\n\\n\\nRemember to be patient and persistent.  LangChain is a powerful but complex framework, so it takes time and effort to master.  Start with the basics, gradually increase the complexity of your projects, and don\\'t hesitate to ask for help when you need it.\\n', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []}, id='run-4317f166-527a-476e-9372-36dfb8b0ee5b-0', usage_metadata={'input_tokens': 39, 'output_tokens': 555, 'total_tokens': 594, 'input_token_details': {'cache_read': 0}})"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "from langchain_core.messages import HumanMessage, AIMessage\n",
        "\n",
        "# Create a message\n",
        "# Message list\n",
        "messages = [\n",
        "    HumanMessage(content=\"Hi\", name=\"Human Student\"),\n",
        "    AIMessage(content='Hi! How can I help you today? \\n', name=\"AI Assistant\"),\n",
        "    HumanMessage(content=\"What is LangChain?\", name=\"Human Student\"),\n",
        "    AIMessage(content='LangChain is a framework for developing applications powered by language models.', name=\"AI Assistant\"),\n",
        "    HumanMessage(content=\"How can I learn\", name=\"Human Student\"),\n",
        "    ]\n",
        "\n",
        "# Invoke the model with a list of messages\n",
        "llm.invoke(messages)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cac73e4c",
      "metadata": {
        "id": "cac73e4c"
      },
      "source": [
        "We get an `AIMessage` response. Also, note that we can just invoke a chat model with a string. When a string is passed in as input, it is converted to a `HumanMessage` and then passed to the underlying model.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "582c0e5a",
      "metadata": {
        "id": "582c0e5a"
      },
      "source": [
        "The interface is consistent across all chat models and models are typically initialized once at the start up each notebooks.\n",
        "\n",
        "So, you can easily switch between models without changing the downstream code if you have strong preference for another provider.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3ad0069a",
      "metadata": {
        "id": "3ad0069a"
      },
      "source": [
        "## Search Tools\n",
        "\n",
        "You'll also see [Tavily](https://tavily.com/) in the README, which is a search engine optimized for LLMs and RAG, aimed at efficient, quick, and persistent search results. As mentioned, it's easy to sign up and offers a generous free tier. Some lessons (in Module 4) will use Tavily by default but, of course, other search tools can be used if you want to modify the code for yourself."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "52d69da9",
      "metadata": {
        "id": "52d69da9"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "TAVILY_API_KEY = userdata.get('TAVILY_API_KEY')\n",
        "os.environ[\"TAVILY_API_KEY\"] = userdata.get('TAVILY_API_KEY')\n",
        "\n",
        "from langchain_community.tools.tavily_search import TavilySearchResults\n",
        "tavily_search = TavilySearchResults(max_results=3)\n",
        "search_docs = tavily_search.invoke(\"What is the Current Weather In Karachi\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "search_docs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JWzZ6zAvb2Fv",
        "outputId": "e0f8edce-61ec-45d9-d906-4ab13fae700f"
      },
      "id": "JWzZ6zAvb2Fv",
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'url': 'https://www.weatherapi.com/',\n",
              "  'content': \"{'location': {'name': 'Karachi', 'region': 'Sindh', 'country': 'Pakistan', 'lat': 24.8667, 'lon': 67.05, 'tz_id': 'Asia/Karachi', 'localtime_epoch': 1732493084, 'localtime': '2024-11-25 05:04'}, 'current': {'last_updated_epoch': 1732492800, 'last_updated': '2024-11-25 05:00', 'temp_c': 24.2, 'temp_f': 75.6, 'is_day': 0, 'condition': {'text': 'Clear', 'icon': '//cdn.weatherapi.com/weather/64x64/night/113.png', 'code': 1000}, 'wind_mph': 6.9, 'wind_kph': 11.2, 'wind_degree': 329, 'wind_dir': 'NNW', 'pressure_mb': 1014.0, 'pressure_in': 29.95, 'precip_mm': 0.0, 'precip_in': 0.0, 'humidity': 75, 'cloud': 0, 'feelslike_c': 26.0, 'feelslike_f': 78.8, 'windchill_c': 24.2, 'windchill_f': 75.6, 'heatindex_c': 26.0, 'heatindex_f': 78.8, 'dewpoint_c': 19.4, 'dewpoint_f': 67.0, 'vis_km': 10.0, 'vis_miles': 6.0, 'uv': 0.0, 'gust_mph': 11.1, 'gust_kph': 17.8}}\"},\n",
              " {'url': 'https://www.easeweather.com/asia/pakistan/sindh/karachi/november',\n",
              "  'content': 'Weather in Karachi in November 2024 - Detailed Forecast Weather in Karachi for November 2024 Your guide to Karachi weather in November - trends and predictions In general, the average temperature in Karachi at the beginning of November is 31.7\\xa0°C. Karachi in November average weather Temperatures trend during November in Karachi Our weather forecast for Karachi in November is based on the analysis of historical data rather than real-time forecast models. Karachi in November - FAQ What to pack to Karachi in November? Below is a carefully tailored packing list to ensure that your time in Karachi, during November, is comfortable and enjoyable, regardless of the weather conditions. November Weather'},\n",
              " {'url': 'https://world-weather.info/forecast/pakistan/karachi/november-2024/',\n",
              "  'content': \"Weather in Karachi in November 2024 (Sind) - Detailed Weather Forecast for a Month Weather World Weather in Karachi Weather in Karachi in November 2024 1 +93°+77° 2 +90°+77° 3 +90°+77° 4 +90°+75° 5 +91°+75° 6 +91°+75° 7 +90°+75° 8 +91°+77° 9 +93°+77° 10 +91°+75° 11 +91°+75° 12 +91°+73° 13 +91°+73° 14 +91°+73° 15 +90°+73° 16 +88°+75° 17 +86°+77° 18 +84°+77° 19 +86°+77° 20 +88°+79° +86°+73° +88°+73° +88°+73° +86°+73° +86°+72° +86°+72° +86°+73° +86°+72° +84°+72° +84°+72° Extended weather forecast in Karachi HourlyWeek10-Day14-Day30-DayYear Weather in large and nearby cities Weather in Islamabad+72° Gwadar+84° Kandhkot+82° Turbat+88° Jacobābād+84° Shikārpur+82° Sukkur+88° Khairpur+88° Shāhdādkot+84° Khuzdār+79° Larkana+84° Kambar+82° Moro+90° Dādu+86° Nawabshah+88° Tando Ādam+88° Tando Allāhyār+88° world's temperature today day day\"}]"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "user = input(\"Enter Your Message? \")\n",
        "\n",
        "result = llm.invoke(user)\n",
        "result"
      ],
      "metadata": {
        "id": "4NvfBxWGnKJa",
        "outputId": "a5fb4682-6987-4f85-fb88-8b03a34d57f3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "4NvfBxWGnKJa",
      "execution_count": 14,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter Your Message? I am HAsnain\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content=\"It's nice to meet you, Hasnain.  How can I help you today?\\n\", additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []}, id='run-d39ee7eb-2f8f-462e-afc9-8434252ca654-0', usage_metadata={'input_tokens': 7, 'output_tokens': 21, 'total_tokens': 28, 'input_token_details': {'cache_read': 0}})"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "P7mA6KOWnJCS"
      },
      "id": "P7mA6KOWnJCS"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}